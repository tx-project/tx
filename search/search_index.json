{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"tx: Transformers x-platform","text":"<p>tx is a simple but powerful cross-platform training library.</p>"},{"location":"#quickstart","title":"Quickstart","text":"<p>Here is an example of how to fine-tune a model on a HuggingFace dataset:</p> <pre><code># Download Qwen3-8B checkpoint\nuv run --with huggingface_hub hf download Qwen/Qwen3-4B --local-dir /tmp/qwen3\n\n# Fine-tune the model on a chat dataset\nuv run --with jinja2 tx train --model Qwen/Qwen3-4B --dataset HuggingFaceH4/ultrachat_200k --loader tx.loaders.chat --split train_sft --output-dir /tmp/ultrachat --batch-size 8 --load-checkpoint-path /tmp/qwen3\n</code></pre>"},{"location":"contributing/","title":"Contributing","text":"<p>We happily accept contributions!</p>"},{"location":"contributing/#development-setup","title":"Development setup","text":"<p>You can run the unit tests with</p> <pre><code>uv run --extra dev pytest -s tests\n</code></pre> <p>You can build and view the documentation by running the following in the project directory:</p> <pre><code>uv run --extra dev mkdocs serve\n</code></pre> <p>The commands in the CI workflow are the best source of useful commands to run during development.</p>"},{"location":"reference/","title":"<code>tx</code>","text":"<p>Usage:</p> <pre><code>$ tx [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <ul> <li><code>--install-completion</code>: Install completion for the current shell.</li> <li><code>--show-completion</code>: Show completion for the current shell, to copy it or customize the installation.</li> <li><code>--help</code>: Show this message and exit.</li> </ul> <p>Commands:</p> <ul> <li><code>train</code>: Train a model</li> <li><code>version</code></li> </ul>"},{"location":"reference/#tx-train","title":"<code>tx train</code>","text":"<p>Train a model</p> <p>Usage:</p> <pre><code>$ tx train [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--model TEXT</code>: HuggingFace model ID or local model path  [required]</li> <li><code>--dataset TEXT</code>: HuggingFace dataset to use for training  [required]</li> <li><code>--loader TEXT</code>: Loader used for loading the dataset  [default: tx.loaders.text]</li> <li><code>--split TEXT</code>: The dataset split to use  [default: train]</li> <li><code>--output-dir PATH</code>: The output directory where the model predictions and checkpoints will be written  [required]</li> <li><code>--load-checkpoint-path PATH</code>: If specified, resume training from this checkpoint</li> <li><code>--save-steps INTEGER</code>: Number of steps between checkpoints  [default: 500]</li> <li><code>--max-steps INTEGER</code>: The maximum number of training steps</li> <li><code>--batch-size INTEGER</code>: Batch size of each training batch  [required]</li> <li><code>--optimizer-args TEXT</code>: Arguments for the optax optimizer  [default: {\"learning_rate\": 1e-5, \"weight_decay\": 0.1}]</li> <li><code>--tp-size INTEGER</code>: Tensor parallelism degree to use for the model  [default: 1]</li> <li><code>--tracker [wandb]</code>: Experiment tracker to report results to</li> <li><code>--tracker-args TEXT</code>: Arguments that will be passed to the experiment tracker (in JSON format)  [default: {}]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/#tx-version","title":"<code>tx version</code>","text":"<p>Usage:</p> <pre><code>$ tx version [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"running/gpus/","title":"Running on GPUs","text":"<p>We assume your are logged into a single GPU node with one or more GPUs. Multi-node instructions will be added later.</p>"},{"location":"running/gpus/#setting-up-tx","title":"Setting up tx","text":"<p>Install <code>uv</code> and clone the <code>tx</code> repository with</p> <pre><code>curl -LsSf https://astral.sh/uv/install.sh | sh\ngit clone https://github.com/tx-project/tx\ncd tx\n</code></pre>"},{"location":"running/gpus/#starting-the-training","title":"Starting the training","text":"<p>Next, download the dataset with</p> <pre><code>uv run --with huggingface_hub hf download Qwen/Qwen3-4B --local-dir /tmp/qwen3\n</code></pre> <p>You can then start the training with</p> <pre><code>uv run --extra gpu --with jinja2 tx train --model Qwen/Qwen3-4B --dataset HuggingFaceH4/ultrachat_200k --loader tx.loaders.chat --split train_sft --output-dir /tmp/ultrachat --batch-size 8 --load-checkpoint-path /tmp/qwen3 --tp-size 8\n</code></pre> <p>In this example we assume you have 8 GPUs in the node. If you have a different number of GPUs, you can modify the <code>--tp-size</code> parameter appropriately.</p> <p>See the full set of options of <code>tx</code> in the CLI reference.</p>"},{"location":"running/tpus/","title":"Running on TPUs","text":"<p>Currently we only have instructions how to run <code>tx</code> on a single TPU VM. Multi-node instructions will be added later.</p>"},{"location":"running/tpus/#setting-up-the-tpu-vm","title":"Setting up the TPU VM","text":"<p>First start the TPU VM:</p> <pre><code>gcloud compute tpus tpu-vm create &lt;TPU_NAME&gt; --project=&lt;PROJECT&gt; --zone=&lt;ZONE&gt; --accelerator-type=v6e-8 --version=v2-alpha-tpuv6e --scopes=https://www.googleapis.com/auth/cloud-platform.read-only,https://www.googleapis.com/auth/devstorage.read_write --network=&lt;NETWORK&gt; --subnetwork=&lt;SUBNETWORK&gt; --spot\n</code></pre> <p>After the VM is started, you can ssh into it via</p> <pre><code>gcloud compute tpus tpu-vm ssh &lt;TPU_NAME&gt;\n</code></pre>"},{"location":"running/tpus/#setting-up-tx","title":"Setting up tx","text":"<p>Once you are logged into the VM, install <code>uv</code> and clone the <code>tx</code> repository with</p> <pre><code>curl -LsSf https://astral.sh/uv/install.sh | sh\ngit clone https://github.com/tx-project/tx\ncd tx\n</code></pre>"},{"location":"running/tpus/#starting-the-training","title":"Starting the training","text":"<p>Next, download the dataset with</p> <pre><code>uv run --with huggingface_hub hf download Qwen/Qwen3-4B --local-dir /tmp/qwen3\n</code></pre> <p>You can then start the training with</p> <pre><code>uv run --extra tpu --with jinja2 tx train --model Qwen/Qwen3-4B --dataset HuggingFaceH4/ultrachat_200k --loader tx.loaders.chat --split train_sft --output-dir /tmp/ultrachat --batch-size 8 --load-checkpoint-path /tmp/qwen3 --tp-size 8\n</code></pre> <p>Note that at the beginning the training is a little slow since the JIT compiler needs to compile kernels for the various shapes.</p> <p>See the full set of options of <code>tx</code> in the CLI reference.</p> <p>You can visualize TPU usage with</p> <pre><code>uv run --with libtpu --with git+https://github.com/google/cloud-accelerator-diagnostics/#subdirectory=tpu_info tpu-info\n</code></pre>"}]}